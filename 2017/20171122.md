-----
**Date : November 22, 2017**

**Venue : Google APAC**

**Group : DataScience SG**

**Topic : Carousell - Machine learning and deep learning**

**Tags : deep learning, NLP, business use case** 

**Speaker : Carousell Data Science Team**

----

*Speaker: Lugas Ngoo, CTO, Carousell*
 
ML applications in carousell: mostly deep learning in the form of computer vision and NLP

- For carousell images are taken in not so professional way - different from other ecommerce 

- chat feature - NLP 

AI first : imagine u can just take a picture and the app will predict category, price etc

*Speaker: Matt Handerson, Principal Data Scientist*<br>
*Topic : joint deep image and language understanding* 

- When u upload a picture to carousell, it will suggest category and titles
- Category is limited but titles have a million different instances 
- They use gpus from google cloud - model trains in 2 days 
- No manual labelling : just use what users already entered
- Dot product ranking : scoring function is the dot product of predictions of both CNN and Ngram embeddings 
- Instead of just generating the text for title, they just rank the titles from a database of whitelisted titles

The vectors for the whitelisted titles are precomputed, so at inference stage, you just need to find out where the image vector is in the space and get the titles closest to that. 

Title biasing: in generating text e.g conversational modelling or chatbots - there is bias towards most likely sequences . E.g : i dont know, i love you

But in case of title ranking, its the opposite problem e.g its overly specific. To overcome this, they use feedback to precompute the biases 

Their joint model outperformed individual models of image and text - it wasnt overfitting. In joint model, the images and text are projected into one, same space 

Summary
- model predicts both category and title in single forward pass.
- faster and more direct than seq-to-seq model 
Paper : efficient natural language response suggestion for smart reply : Handerson et.al for more on dot product ranking 

Metrics for title suggestion : 1 in 100 accuracy

*Speaker: Hoang Nguyen, ML engineer*<br>
*Topic: Conversational modelling for carousell chat*

Millions of chat message 
Each transaction has listing features and text messages 

Take every pair of messages : previous message + response 

Same dot product ranking is used except the two components here are previous message and response (instead of image and title) 

Diversity threshold to tell the model to pick up less similar suggestions 

For improvement : use upto 20 previous messages instead of just the last one 

Also they categorise the message into few types : seen type, response type etc 

Metrics: message 1-of-100 accuracy

The previous one is bag of words. They also tried RNN but without much improvement. Maybe because they have fixed training time for all models and rnn requires more training time.

This is like stepping stone for carousell assistant

Q and A

Pretraining : it helps to use pretrained when they dont have much data but once you have a lot of data from their app, then better to train on your own data by scratch 

- for CNN architecture, they use mobilenet as training time is very fast 

- for visualisation , they just use tsne (look at the last picture u took)

Tech stack : python, tensorflow, google cloud ML, BigQuery 