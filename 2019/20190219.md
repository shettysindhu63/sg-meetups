-----
**Date : February 19, 2019**

**Venue : DBS Auditorium**

**Group : Singapore Analytics Freelancers**

**Topic : Exploring the Fourth Dimension of Analytics**

**Tags : data science, analytics** 

**Speaker : Stephen Brobst**

----

From *Jim Gray The Fourth Paradigm*

1. Emperical discovery
2. Theoretical discovery : mathematical characterisation 
3. Computational discovery - simulation
4. Data discovery - sensors measuring, observe the phenomena - this is the fourth paradigm - data intensive science 

Everything will be sensor enabled 

Schema on load vs schema on read 

- 70% of work on data modelling, ETL (schema on load) - this better for BI analysts - finding answers to business questions - lots of people in the business

- late binding (schema on read) - u pushed the data modelling part further ahead of EDA - u get hands on  data earlier - better for data scientists who are usually sophisticated enough to work through the data and mapping - data scientists find the questions that need to be answered. Work with a lot of JSON. Data engineers to work with data scientists for deployment. 

Data Discovery

1. Capture  - lots of funding 

2. Curation - data housework, cleaning the closet, metadata (data about data) : we are terrible at this, need to get better at this 

3. Analysis - data science doing ok, although supply still less than demand 

Data curation

90% data in datalakes useless - Gartner report
Measure of success right now is size of datalake - this is the problem

Comparing with encyclopedia vs wikipedia 

- crowdsource the metadata from your data scientists - it doesnt need to be perfect - it just needs to get better with time when more and more data scientists refine it (like wikipedia)
- when finally u need to publish/deploy to general users - then u cross check and curate more carefully 

Data Provenance 

- Keep track of the data and whats happened to the data : Data Provenance 
- Basic questions of Provenance - (Ref: visualisation for data intensive science) 

Value of data increases when it is combined with other interesting data 

Facilitate fast and cheap failing 
- u need to run different experiments fast and fail fast 
- data scientists live in discovery 
- needs agility and flexibility

Integrated data warehouse 
- data engineers 
- automate and remove dependencies on single person, no manual processes 
- reliability and productising the data products to monetise them 

You need both data scientists and data enginerers to work together 

Use the agile approach but its not good enough 
- extend agile to DevOps 
- continuous integration
- Data is different: theres more work to be done in sort of analytic Ops in data centric environments 

Better metrics for data lake (size is definitely wrong)

- how fast can u transfer this data to the data scientists 
- how much value : valuable data is shifted from data lake to data warehouse 

Good resource for learning about late binding for metadata: blogs on late binding, JSON 
