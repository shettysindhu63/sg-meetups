-----
**Date : April 12, 2018**

**Venue : NUS SoC**

**Group : NUS SoC and Telecom ParisTech**

**Topic : Joint Workshop on Data Sciences and Artificial Intelligence**

**Tags : machine learning, streaming data** 

**Speaker : Too many**

----

*Speaker: Prof. Albert Bifet, ParisTech*<br> 
*Topic : Massive Online Analytics for the Internet of Things*  

Group: Data Intelligence and Graphs (DIG)

Machine Learning for Data Streams 

- IoT applications for energy management, smart cities, ind 
- applications IoT analytics (picture taken from IoT analytics https://iot-analytics.com/ )
- IOT and industry 4.0 (comes from german government)

ML is a type of AI that provides computers with the ability to learn without being programmed explicitly 

AI systems - concept and definition - by Nikola Kasabov (https://www.aut.ac.nz/profiles/nikola-kasabov)

Standard approach: Finite training sets, static models 

This cannot be applied to streaming data 

Data stream approach : every time data arrives, update model continuously
Infinite training sets, dynamic models 

So when data is changing over time, model will update on its own 

Adversarial Learning 

AI challenges 
Report : for a meaningful artificial intelligence, Cedric Villani
https://www.aiforhumanity.fr/pdfs/MissionVillani_Report_ENG-VF.pdf

The following from the report 

1. Green AI 
- only one pass over data 
- approximate algorithms 

One of the first papers on approximate algorithms : Counting large number of events in small registers 

2. Explainable AI 
- Focus on decision tree 
decision tree is not a streaming method 

so what is the alternate to this ? we wait for to collect 
Hoeffding Tree is becoming popular for this - check TensorForest for practical implementation 
http://huawei-noah.github.io/streamDM/docs/HDT.html
Sample of stream enough for near optimal decision 

Adaptive random forests 
Adaptive random forests for evolving data stream classification
this is current state of art : https://link.springer.com/article/10.1007/s10994-017-5642-8

3. privacy issues, ethical issues 
should data have an expiration data ?
- dont store data, just store models ? 
- in streaming data cases, you may be able to mitigate these ethical issues 

MOA - open source project, related to weka 
CHECK THIS 
https://github.com/Waikato/moa 

Open book available online : https://mitpress.mit.edu/books/machine-learning-data-streams

OpenML website 

Another challenge : Distributed Data Mining in Streams 
how to do this in clusters, where to deploy
Apache SAMOA
S4, Storm, FLINK 
FLINK - is like Spark but for streaming purposes 
they also use apache spark (spark is built on scala)
Vertical Partitioning 

Learning fast and slow 
based on book thinking fast and slow

In many applications, you need deep learning but in streaming you go through data only once, deep learning requires many iterations through data 

System 1 : fast thinking - streaming ML , fast 
System 2:  slow thinking - deep learning, more accurate

Combine the two systems and use the better one when you want to make predictions 

At the beginning fast system can start predicting and after certain time, you use deep learning which has stored data and trained on it 

CHECK : scikit-multiflow , Python 
https://github.com/scikit-multiflow/scikit-multiflow

----

*Speaker: Koo Sengmeng* <br> 
*Topic : AI Singapore*  

https://www.aisingapore.org/
https://kelaberetiv.aisingapore.org/

agencies involved : EDB, IMDA, NRF, MOH holdings, SG Innovate,Smart Nation 
AI Singapore is a 5 year research program -  initial 150M funding 

Strategic Thrusts 

1. AI innovation - headed by Lawrence Liew (I think I know this person)
100E -100 experiments 
must be relevant to Singapore or Singapore home grown companies
matching between AI user (industry) and AI performer (researchers)
In this, they don't take very early companies (this is handled by data science consortium) but they collaborate with companies that already have experience in data analytics and they have cutting edge use case 

AI Apprenticeship Programme (AIAP) - only for Singaporeans 
(but they currently have lack of mentors) - the training for this is done by NUS Scale http://scale.nus.edu.sg/ 
graduates with less than 3 years work experience 
he talk about kubernetes, pachyderm, tensorflow 
training content include: data visualisation, data engineering, big data engineering,

Online collaborative platform : Kelaberetiv - encourage researchers to put up online profiles for collaboration with industry 

2. AI technology  (80% money is alloted here)
national research long term goals 
they will be announcing AI grand challenge for healthcare soon - 3 H high blood pressure, high blood sugar, hypertension  - do diagnosis early on and intervene for prevention 
Healthcare, Urban Solutions, Finance  

they are talking about homographic encryption, GDPR, pan country data sharing  - these are in regard to healthcare data 

3. AI Research 

Looking at techniques that is relevant to Singapore 
AlphaGo, YOLO, explainable AI, reproducible AI

For forseeable years, there will still be man-in-loop as they don't want discrimination in applied AI 
regulators are looking at explaining the decisions or reproducing 

National Language Processing: they are developing a singlish corpus - very important to NLP, government is interested in local chatbots 
Computer vision : replicate alibaba city brain project 

----

*Speaker: Toh Swee Feng Caroline* <br> 
*Topic : Singapore Data Science Consortium*  

SDSC - by NUS, NTU, SUTD, astar funded by NRF 

Singapore mostly has lot of SMI (small medium industries) that dont have the basic infrastructure for data science 

These companies mostly have data analytics/BI tools/SAP tools but they dont have coding, data science, ML expertise 

They provide reskilling/training for both technical and 

Industry-professor matching 

SDSC has a pool of data scientists 

SDSC sponsers industry events where professors can give talks

There are a lot of courses 

SDSC also hires data scientists to be placed into the projects 

----

*Speaker: Yair Zick, NUS SoC*<br> 
*Topic : Diversity Constraints in Public Housing Allocation* 

Abstract: The state of Singapore operates a national public housing program, accounting for over 80% of its residential real estate. Singapore uses its housing allocation program to ensure ethnic diversity in its neighborhoods; it does so by imposing ethnic quotas: every ethnic group must not own more than a certain percentage in a housing project, thus ensuring that every neighborhood contains members from each ethnic group. However, imposing diversity constraints naturally results in some welfare loss. Our work studies the tradeoff between diversity and social welfare from the perspective of computational economics. We model the problem as an extension of the classic assignment problem, with additional diversity constraints. While the classic assignment program is poly-time computable, we show that adding diversity constraints makes the problem computationally intractable; however, we identify a 1/2-approximation algorithm, as well as reasonable agent utility models which admit poly-time algorithms. In addition, we study the price of diversity: this is the loss in welfare incurred by imposing diversity constraints; we provide upper bounds on the price of diversity as a function of natural problem parameters; next, we analyze public data from Singapore’s Housing and Development Board, and create a simulated framework testing the welfare loss due to diversity constraints in realistic large-scale scenarios.

Talk notes 

Public housing estates 

Ethnic integration policy to prevent de facto segregation 
Introduced 1989, revised 2010

Each block 
Chinese < 87%
Malay < 25%
Indian/others < 15% 

other examples of this scenario - assigning kids to kindergartens 

agents N (people) ,items M (blocks)

Every agent has a utility function 
Each block has  type-capacity constraint based on the ethnicity 

Assignment with type constraints is computationally intractable 

Then they do some approximations so that they can find optimal assignment in polynomial-time special case 

Uniformity breeds Simplicity 

The price of diversity - social welfare -here by worse social welfare, he means people are not able to get apartments anymore because of these constraints 

Distance-based or ethnicity-based utility functions 

They dont have actual data, very difficult to get this data

This is more of a simulation or experiment 

what does deversity even mean ?  

----

*Speaker: Reza Shokri, NUS SoC*<br> 
*Topic : Privacy in Machine Learning as a Service*<br> 

Abstract: In this talk, I will present different threats against data privacy in machine learning systems, with the focus on machine learning as a service platforms. Google, Amazon, Microsoft, BigML, and other service providers enable data holders to benefit from machine learning as a service by simply uploading their data to the service provider and obtaining API access to machine learning models trained on their data.

Obviously, the service providers get direct access to the data, which may be of a serious concern for sensitive data holders. I show how machine learning as a service platforms could be constructed that enable service providers to train models without seeing the data. Even with a blind training the model is not privacy preserving. The threat also extends to the users of the machine learning as a service. I will show that an attacker (who accesses the model through the machine learning API) can build inference algorithms to determine the members of the model’s training ata. Finally, I will discusss how such subtle threats could be mitigated.

TALK NOTES 

Machine learning as a Service 

The service providers do not provide the users the model but just the training API and prediction API

you have to upload your data to service provider - this is obviously a privacy issue 

Training on encrypted data ? 
- homomorphic encryption 
- end result only you can decipher, service provider cannot 
- secure 
- very heavy in time and storage
- not yet ready or mature for large scale machine learning 

Trust the hardware 
- you upload encrypted data 
- to disclose data only to the trusted algorithm 
- trust computing 
- intel SGX (software gaurd extension)
- only trusted code can access data 
- data upload and query from prediction is encrypted 

Will there be information leakage ?

"Leakage" - being able to learn information about the training data, which is not learned from models/other data

One leakage scenario - membership inference attack 

Exploit model's predictions 
main insight: ML models overfit to their training data 
to recognise the difference and check if member of training data - the attacker can use some adversarial model 

train attack model using shadow models 

this attacks really work - 85% to 90% accuracy

privacy: whether the model leaks information about data in the training set
learning: does model generalise to data outside the training set
common enemy: overfitting 

if model is regularised, this reduces the risks

what assumptions are made about the attacker?
- attacker has access to the same platform, when he trains shadow models 

privacy-preserving models that dont affect utility i.e prediction accuracy are preferred

Other approaches 
Differential privacy 
  
----

*Speaker: James Eagan, DIVA group, ParisTech*<br> 
*Topic : Reinventing the mind’s bicycle*  

Topic related to human computer interaction 

Sense-making 

Abstract: Steve Jobs once described the computer as “a bicycle for the mind”, a tool that amplifies people’s cognitive capabilities. My work focuses on making this bicycle a better, more expressive tool for humans in a data-rich world: by providing richer tools to interact with and understand data, by making interactions more expressive, and by reinventing how we build software so users can appropriate and adapt it to their own idiosyncratic needs. In this talk, I will present an overview of our work in these three areas.


TALK NOTES 

how to navigate large systems diagrams 

Visualisation:  SchemeLens: A Content-Aware Vector-Based Fisheye Technique for Navigating Large Systems Diagrams.

France has lot of nuclear plants and the command control room has lot of complex plant diagrams 

A force-based model is used instead of naive vertical zoom 

Sense-making 
facebook connections of the world picture 
data workers : people who routinely analyse data but do not conider themselves data scientists 
How would they interpret uncertainity ?

the goal is to inform the designers to design better tools for sense-making 

Scotty toolkit - sense-making tools 

they have some interactive python prompt 
one more they hack into itunes to add subtitles 

software is not soft 
still difficult to adapt software to your needs 

can we make software malleable ? 

they hacked into cocoa (something for mac) and integrated hooks into it 

