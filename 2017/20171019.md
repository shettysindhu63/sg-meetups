-----
**Date : October 19, 2017**

**Venue : Accenture Digital Hub**

**Group : Data Science SG**

**Topic : Music Composition using Neural Networks**

**Tags : deep learning, neural networks** 

**Speaker : Shen Ting Ang, Wecash SEA**

----

- How to model music ? 
- RNN vs LSTM ? - u expect LSTM to be better . If LSTM, which gating to use : test/compare 

- very difficult to model raw waveforms. They tried this with Bach music - but loss wouldnt converge even after 2000 iterations 

Common feature representation for waveforms : Mel frequency Cepstrum Coefficients (MFCC)

Notation representation - ABC which is like a compact, textual representation of MIDI 

- there are readily available datasets for ABC music 

They decided to ditch the MFCC for ABC notation as there is already well studied text generation problem using RNN 

Read : Character-level RNN text generation - Andrew Kaparthy 

Tools : python with keras , about 100 lines of code, modified text generation example 

- good convergence rate : 100-200 iterations 

- LSTM had lower loss than RNN, they compare optimizers (RMSprop, SGD), different number of  layers 

Big question : How to evaluate ?

- how to objectively evaluate music generated ? 

- Eulers Gradus Suavitatis invented in 1739- measure of melodiusness - higher is better 

- Subjective evaluation by humans : rate samples from 1-10 on harmony, melodiusness and do you think this is created by computer? 

Deepmind - wavenets - works on raw audio 