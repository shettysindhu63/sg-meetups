-----
**Date : November 21, 2017**

**Venue : SG Innovate**

**Group : Tensorflow and Deep Learning**

**Topic : Tensorflow basics and transfer learning**

**Tags : tensorflow, transfer learning, deep learning** 

**Speaker : Sam Witteveen and Martin Andrews, Google**

----

Speaker: Sam Witteveen

Tensors - multi dimensional arrays 

Inception V3 model for image classification - state of the art 2 yrs ago - google provides this model out of the box 

In tensorflow - after you write the python program - it gets converted to an optimised (for ur problem) tensorflow graph 

Tensorboard  - for visualization - tsne . U cant use keras with tensorboard yet, but may come very soon

Tensorflow serving : separate package to serve tensorflow models to the cloud directly 

XLA : tensorflow  compiler - take model in ur comp and covert it into models that r optimized to run on any device - e.g on wearables etc. 

Read about this :Japanese cucumber sorting application 

If u r staring out : use keras instead of directly 

As of tensorflow 1.4 - keras is optimised and built into core

Estimators

He also talks abt estimators - like scikit learn 
Canned estimators - easiest thing to start with but not flexible - out of the box model 

Estimator framework is different frm keras. You dnt need to know any other wrappers other than keras or estimators 

Estimators does away with loops - more optimised - build models using estimators - export the model easily - serve the model and scale it 

Cannned estimators 
Quite similar to scikit learn framework 
Since model part is just one line- you can try out different models very quickly to compare 
It s possible that some standard machine learning algos may be available soon 

Some code examples using canned estimators - check it online : always start out with mnist because it trains very quickly - u wont waste time 
 
Most of the algos that google uses are standard neural networks. 

Speaker : Martin Andrews

Deep learning workshop : [https://github.com/mdda/deep-learning-workshop](https://github.com/mdda/deep-learning-workshop)

Transfer learning using pretrained models 

Use state of the art tensorflow model, solve a problem it wasnt trained on , use deep learning as a component 

Deep learning : feature engineering at the expense of model explainability or interpretibiliy

Imagenet : now 4% error rate better than humans 

Take an existing model trained for inagenet, 
Instead of softmax layer - use logits layer as features to train an SVM , svm is well understood so good for interpreting. SVM work very well for specific things 

Hes using say 10 set of features of either classes and then train SVM on it.

It is important to remenber it actually works and dont need to train a deep neural network but need to undestand that model may not know exactly what its doing