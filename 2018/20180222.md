-----
**Date : February 22, 2018**

**Venue : Google APAC**

**Group : Tensorflow and Deep Learning**

**Topic : NIPS 2017 Review**

**Tags : deep learning, AutoML** 

**Speaker : Four speakers**

----

*Speaker: Martin Andrews*<br> 
*Topic : Deep Learning Practices and Trends*  

- slides in the meetup comments section with links to video, slides of the tutorial - http://redcatlabs.com/2018-02-22_TFandDL_NIPS-review/#/

Tutorial: Architectures, building blocks  
- CNNs/RNNs

CNN for images 
- exploit translational invariance : image shifts does not matter, objects close together do matter 

RNN for sequences 
- exploit time invariance, order of words matter but not the location within the sentence 
- word embeddings : convert words to vectors, based on context; first step in any deep NLP model 
- seq2seq , encoding -> decoding 
- attention mechanisms 

Tutorial : Trends 

Extending on Invariance 
- Networks for graphs 
- Networks for programs : two directions - neural network learns to do the task, neural networks writes a program to do the task 
- Network for networks : exploits 'problem solving' invariance, type of task matters but not the underlying data, two directions - one-shot/few shot learning, learn to make better networks 

Autoregressive models 
- important type of generative model 
-  approach : dilated convolutions 
- distribution learning 
- WaveNet MOS : teacher-student network - enables wavenet in production 

Domain alignment 
- unsupervised learning, latent spaces , different domains mapped to the same space 
- personal note: i guess this is one of the areas where the gwern anime dataset may be useful
- faces and cartoons 
- cycle consistency ->CycleGAN 
- GRASPGAN - synthetic images more representative of the real world for simulation  
- unsupervised translation 

FREE from google - Colab free GPU for 8/12 hrs 

----

*Speaker : Olzhas Akpambetov*<br>
*Topic: Intro to Capsule networks* 

An overview of dynamic routing between capsules 

How CapsNet different from ConvNets 
- inverse graphics 
- good results without data augmentation
- achieves positional invariance 
- convnets use max pooling : this is an inefficiency 

this is some complicated stuff, refer to the slides in the meetup comments section, has some resources to understand this better 

New routing : Matrix capsules with EM routing - new routing mechanism with better results 
Capsule networks vs adversial - for defending neural networks against white box attacks etc

Paper - capsule network performance on complex dataset (CIFAR10)
Other papers - capsnet for DQN reinforcement learning, CapsuleGAN

Aurelion Geron has a youtube video tutorial, notebook with code 

----

*Speaker : Chaitanya Joshi*<br> 
*Topic: Personalization in Goal-Oriented Dialog, Conversational agents* 

Motivation : Personalisation of a dialogue system's response based on who its interacting with 
- no open datasets to investigate this, but companies will have a lot of propriety data 

Key contributions 
- new datasets of goal-oriented dialogs influenced by speaker profiles 
- modified memory network architecture to tackle personalisation 

they started with the facebook bAbI dialog tasks - this is a synthetic data - you want the model to learn individual tasks- to test how 

memory networks are easy to visualise - how the model is learning about the language  

In this paper, they start from the bAbI dialog tasks and added a layer of personalisation by creating user profiles 

Memory networks for dialog 
input - what user is saying 
output - what bot is responding with 
memory module - everything about the dialog till then along with a controller and attention module 

In the paper - they introduced a small modification - split memory with another module for reasoning over user profiles 

visualising attention - compare with the standard memory networks 

limitations 
- dealing with entities 
github repo - chaitjo/personalised-dialog 

New stuff
- Facebook AI released a new dataset with real conversions with mechanical turkers 
- microsoft research has some new dialog models 
- new research on evaluation, metrics 

----

*Speaker : Sam Witteveen*<br>
*Topic : Meta Learning, AutoML, NASnet, Learning to Learn*

Meta Learning 
- the framework of using one learning system to modify or optimise certain aspects of another learning system 
- terms : learning to learn, AutoML 
- most approaches right now focus on automating the process of choosing architectures and hyperparameter tuning which currently is more of an art 
- grid search in scikit-learn similar to meta learning but its more in a brute-force fashion 
- meta learning is a smarter way of doing this and at a very large scale 

4 ways
- evolutionary optimisation
- bayesian optimisation 
- gradient descent 
- reinforcement learning 

Parts of meta learning 
architecture : the optimizer - controller network, child network - optimizee 

Neural Architecture Search 
- The controller is an RNN network, this will process the config file 

NAS for CiFAR 10 
- 800 GPUs concurrently !!!!
- parameter serer : controller network and child networks 
- in total they trained 12800 models 
- final child network is the result and the result is something that a human would not come up with  
- gets state of the art results 

NAS for ImageNet 
- No of GPUs requird would be impractical 

- the goal is to get a particular cell like the inception cell that can then be repeated - you get the NASnet cells (normal cells, reduction cells )
- with this approach, NASnet gets the best results right now with almost half the number of parameters 

New paper : Efficient neural architecture search 
- reduce compute required by 1000x
- does not train from scratch : it shares weights from previous iterations 

Google Cloud AutoML 
- upload your images with the labels 
- it will come up with the best network for this set 

Refer to the key papers/articles, NASnet implementations 